# Server Monitoring Roadmap

This roadmap outlines the development plan to transform the current basic monitoring system into a comprehensive, production-ready monitoring platform with persistence, visualization, and service health checks.

## Current State (v0.5.0)

âœ… **Implemented:**
- Agent-hub architecture for distributed monitoring
- Real-time CPU usage and temperature monitoring
- Configurable thresholds with grace periods
- Alert system (Discord webhooks, generic webhooks)
- Authentication via tokens
- Actor-based architecture with Tokio actors
- SQLite persistence with configurable retention
- Service health monitoring (HTTP/HTTPS)
- REST API + WebSocket streaming
- TUI dashboard with time-based charts
- Historical data loading and uptime tracking

âœ… **Architecture:**
- Clean actor-based design (Collector, Storage, Alert, ServiceMonitor)
- Broadcast channels for event distribution
- Graceful shutdown and supervision
- Pluggable storage backends (SQLite, in-memory)

ğŸ¯ **Next Focus:**
- Performance optimization and benchmarking
- Production hardening and observability
- Documentation and deployment guides
- Release binaries and distribution

## Vision (v1.0.0)

A comprehensive monitoring platform featuring:
- ğŸ“Š Beautiful terminal UI with real-time graphs
- ğŸ’¾ Time-series metric storage with configurable retention
- ğŸŒ Service uptime monitoring (HTTP/HTTPS + ICMP ping)
- ğŸ”Œ REST + WebSocket API for remote access
- ğŸ—ï¸ Clean actor-based architecture
- ğŸ“ˆ Historical trend analysis and reporting

---

## Phase 1: Architecture Refactoring ğŸ—ï¸ [âœ… COMPLETE]

**Goal:** Modernize the codebase with a clean actor-based architecture

**Duration:** 1-2 weeks

**Status:** Complete - All actors integrated with graceful shutdown âœ…

### 1.1 Actor Model Design
- [x] Design actor system with clear responsibilities âœ…
  - `MetricCollectorActor` - polls agents and collects metrics âœ…
  - `StorageActor` - handles all persistence operations âœ…
  - `AlertActor` - evaluates rules and sends alerts âœ…
  - `ServiceMonitorActor` - monitors service health âœ…
- [x] Define message types and communication patterns âœ…

### 1.2 Channel Architecture
- [x] Replace current loop-based polling with tokio channels âœ…
- [x] Implement `mpsc` channels for actor commands âœ…
- [x] Implement `broadcast` channels for metric events âœ…
- [x] Add backpressure handling and buffering strategies âœ…

### 1.3 Hub Refactoring
- [x] Refactor `hub.rs` to spawn actor tasks âœ…
- [x] Implement graceful shutdown for all actors âœ…
- [x] Create unified configuration system âœ…

### 1.4 Testing & Migration
- [x] Add basic unit tests for actors (29 tests) âœ…
- [x] Add integration tests for actor communication (43 tests) âœ…
- [x] Ensure backward compatibility with existing configs âœ…

**Dependencies:** None
**Deliverables:** Cleaner, more maintainable codebase with actor model
**Reference:** [docs/architecture/REFACTORING_PLAN.md](docs/architecture/REFACTORING_PLAN.md)

**Progress Notes:**
- **2025-01-15**: Created actor module structure (`src/actors/`)
  - Implemented `MetricCollectorActor` - replaces old `server_monitor` loop
  - Implemented `AlertActor` - maintains grace period state machine
  - Implemented `StorageActor` (in-memory stub for Phase 2)
  - Added message types (`MetricEvent`, commands for each actor)
  - Set up broadcast channel for metric distribution
  - All tests passing (5/5) âœ…

---

## Phase 2: Metric Persistence ğŸ’¾ [âœ… COMPLETE]

**Goal:** Add time-series storage with flexible backend options

**Duration:** 1-2 weeks

**Status:** Complete - SQLite backend with batching and retention âœ…

### 2.1 Storage Abstraction
- [x] Design storage trait with CRUD operations âœ…
- [x] Define metric schema (timestamp, server_id, metric_type, value, metadata) âœ…
- [x] Implement batch write operations âœ…
- [x] Add query interface for time ranges and aggregations âœ…

### 2.2 Backend Implementations
- [x] **SQLite backend** (default, embedded) âœ…
  - [x] Schema design with indexes âœ…
  - [x] Migration system (sqlx) âœ…
  - [ ] Connection pooling (using single connection currently)
- [ ] **PostgreSQL backend** (optional, production - Phase 2.5)
  - TimescaleDB extension support
  - Hypertable configuration
  - Continuous aggregates
- [ ] **Parquet file backend** (optional, archival - Phase 2.5)
  - Columnar storage with compression
  - Partition by time (daily/hourly files)
  - Efficient range queries

### 2.3 Retention & Aggregation
- [x] Configurable retention policies (implemented in Phase 4.0) âœ…
- [x] Automatic data pruning/archival (implemented in Phase 4.0) âœ…
- [ ] Downsampling for long-term storage (1min â†’ 5min â†’ 1hr) â†’ **Future enhancement**
- [ ] Query optimization for large time ranges â†’ **Future enhancement**

### 2.4 Integration
- [x] Update `StorageActor` to persist all metrics âœ…
- [x] Add configuration for storage backend selection âœ…
- [x] Add storage health checks âœ…

**Dependencies:** Phase 1
**Deliverables:** Persistent metric storage with multiple backend options
**Reference:** [docs/features/METRIC_PERSISTENCE.md](docs/features/METRIC_PERSISTENCE.md)

**Progress Notes:**
- **2025-01-15**: SQLite backend implementation complete
  - Created `StorageBackend` trait with async operations
  - Implemented `SqliteBackend` with WAL mode and optimized indexes
  - Designed hybrid schema: aggregate columns (indexed) + complete metadata (JSON)
  - Added batching strategy: dual flush triggers (100 metrics OR 5 seconds)
  - Extended `StorageActor` with `Option<Box<dyn StorageBackend>>` for persistence
  - Configured via `storage` section in config (SQLite or in-memory)
  - All tests passing (84/84) âœ…
  - Backward compatible: falls back to in-memory if no storage configured
- **2025-01-16**: Retention and cleanup complete (Phase 4.0)
  - Added `retention_days` and `cleanup_interval_hours` configuration
  - Implemented background cleanup task in `StorageActor`
  - Cleanup runs on startup and at configured intervals
  - Statistics tracking: last cleanup time, metrics/checks deleted

---

## Phase 3: Service Monitoring ğŸŒ [âœ… COMPLETE]

**Goal:** Add HTTP/HTTPS endpoint monitoring and ICMP ping support

**Duration:** 1 week

**Status:** Complete - Service monitoring with persistence and alerts âœ…

### 3.1 HTTP/HTTPS Monitoring
- [x] Design service check configuration schema âœ…
- [x] Implement HTTP client with timeout/retry logic âœ…
- [x] Support multiple HTTP methods (GET, POST, HEAD) âœ…
- [x] Validate response codes, headers, body patterns âœ…
- [x] Measure response time and SSL cert expiration âœ…
- [x] Track consecutive failures for alerting âœ…
- [ ] ICMP Ping Monitoring (deferred to v1.1.0)

### 3.2 Service Status Tracking
- [x] Add service state machine (UP/DOWN/DEGRADED) âœ…
- [x] Implement grace periods for flapping detection âœ…
- [x] Store service check history (SQLite + in-memory) âœ…
- [x] Generate uptime percentage calculations âœ…

### 3.3 Alert Integration
- [x] Extend alert system for service failures âœ…
- [x] Add service-specific alert templates (Discord + Webhook) âœ…
- [x] Include error messages in alerts âœ…
- [x] Support status transitions (down â†’ recovery) âœ…

### 3.4 Storage & Query API
- [x] ServiceCheckRow schema with persistence âœ…
- [x] Public query API in StorageHandle âœ…
  - `query_service_checks_range()` - time range queries
  - `query_latest_service_checks()` - latest N checks
  - `calculate_uptime()` - uptime statistics
- [x] Integration tests (persistence, uptime, range queries) âœ…

**Dependencies:** Phase 1, Phase 2
**Deliverables:** Comprehensive service health monitoring
**Reference:** [docs/features/SERVICE_MONITORING.md](docs/features/SERVICE_MONITORING.md)

**Progress Notes:**
- **2025-01-15**: Service monitoring implementation complete
  - Created `ServiceMonitorActor` with configurable check intervals
  - Implemented HTTP/HTTPS health checks with method, body pattern, header validation
  - Added `ServiceCheckEvent` messages published to broadcast channel
  - Extended `StorageActor` to persist service checks to SQLite
  - Implemented `send_service_alert()` in AlertManager (Discord + Webhook)
  - Added uptime calculation with SQL aggregation (percentage, avg response time)
  - Created public query API in StorageHandle for dashboard/API access
  - All tests passing (84/84: 29 unit + 43 integration + 9 property + 3 doc) âœ…
  - ICMP ping monitoring deferred to future release (requires elevated permissions)

---

## Phase 3.5: Alert Architecture Refactoring ğŸ””

**Goal:** Split metric and service alert managers for cleaner architecture

**Duration:** 3-5 days

**Priority:** Medium (after Phase 4.1 - do after retention cleanup and basic API)

### 3.5.1 Design Alert Abstraction
- [ ] Design `AlertSender` trait for shared Discord/Webhook delivery logic
- [ ] Define interface for `MetricAlertManager` (CPU, temp, disk, memory)
- [ ] Define interface for `ServiceAlertManager` (uptime, SSL, response times)
- [ ] Plan migration path from current `AlertManager`

### 3.5.2 Implementation
- [ ] Implement `AlertSender` trait with Discord and Webhook backends
- [ ] Extract `MetricAlertManager` from current `AlertManager`
- [ ] Extract `ServiceAlertManager` from current `AlertManager`
- [ ] Update `AlertActor` to use separate managers
- [ ] Remove old `AlertManager` once migration complete

### 3.5.3 Testing & Documentation
- [ ] Update unit tests for new architecture
- [ ] Update integration tests for alert flows
- [ ] Document alert manager selection logic
- [ ] Add examples for custom alert types

**Why This Refactoring:**
- Current `AlertManager` handles two conceptually different domains (metrics vs services)
- Different alert patterns: metrics use thresholds, services need SLA tracking
- Easier to add new alert types in the future (log alerts, security alerts)
- Better separation of concerns and testability

**Why Not Urgent:**
- Current implementation works without bugs or performance issues
- More critical features needed first (retention, dashboard)
- Will understand pain points better after real-world usage

**Dependencies:** Phase 3
**Deliverables:** Cleaner alert architecture ready for future extension

---

## Phase 4: Dashboard & API ğŸ“Š [âœ… COMPLETE]

**Goal:** Build TUI dashboard and remote API access

**Duration:** 2-3 weeks

**Status:** Complete - Full API and TUI dashboard implemented âœ…

### 4.0 Retention & Cleanup [âœ… COMPLETE]
- [x] Implement background task for automatic data pruning âœ…
- [x] Add configurable retention policies per metric type âœ…
- [x] Cleanup old metrics on hub startup âœ…
- [x] Add retention statistics to storage stats âœ…
- [x] Add metrics for cleanup operations (rows deleted) âœ…

### 4.1 API Server (Axum) [âœ… COMPLETE]
- [x] Design REST API specification âœ…
  - `GET /api/v1/health` - health check
  - `GET /api/v1/stats` - system statistics
  - `GET /api/v1/servers` - list all monitored servers with health status
  - `GET /api/v1/servers/{id}/metrics` - query metrics with time range
  - `GET /api/v1/servers/{id}/metrics/latest` - latest N metrics
  - `GET /api/v1/services` - list all monitored services with health status
  - `GET /api/v1/services/{name}/checks` - service check history
  - `GET /api/v1/services/{name}/uptime` - uptime statistics
- [x] Implement request authentication/authorization (Bearer token) âœ…
- [x] WebSocket endpoint for real-time metric streaming (`/api/v1/stream`) âœ…
- [x] CORS support for web dashboards âœ…

### 4.2 WebSocket Streaming [âœ… COMPLETE]
- [x] Implement `tokio-tungstenite` WebSocket handler âœ…
- [x] Subscribe to metric broadcast channel âœ…
- [x] Subscribe to service check broadcast channel âœ…
- [x] Filter and serialize events for clients âœ…
- [x] Handle client reconnection and buffering âœ…

### 4.3 TUI Dashboard (Ratatui) [âœ… COMPLETE]
- [x] Initialize Ratatui with Crossterm backend âœ…
- [x] Implement tabbed interface layout âœ…
  - **Servers Tab:** Server list + detailed metrics with time-based charts
  - **Services Tab:** Service health status with check history
  - **Alerts Tab:** Alert timeline with severity indicators
- [x] Create chart components with time-based X-axis (HH:MM:SS) âœ…
- [x] Enhanced system info panel (hostname, OS, architecture) âœ…
- [x] Color-coded memory gauges with progress bars âœ…
- [x] Implement real-time updates via WebSocket âœ…
- [x] Add interactive controls (pause, refresh, navigation) âœ…
- [x] Sliding time window for charts (configurable, default 5 minutes) âœ…
- [x] Historical data loading on startup âœ…

### 4.4 CLI Binary (`guardia-viewer`) [âœ… COMPLETE]
- [x] Create new binary in `src/bin/viewer.rs` âœ…
- [x] Support connection to local or remote hub âœ…
- [x] Configuration file for API endpoint and auth (`~/.config/guardia/viewer.toml`) âœ…
- [x] Graceful error handling and automatic reconnection âœ…
- [x] Help text and keybindings display âœ…
- [x] CLI arguments for URL and token override âœ…

**Dependencies:** Phase 1, Phase 2, Phase 3
**Deliverables:** Beautiful TUI dashboard and flexible API âœ…
**Reference:** See CLAUDE.md for detailed architecture documentation

**Progress Notes:**
- **2025-01-16**: Phase 4.0 (Retention & Cleanup) complete
  - Background task for automatic metric/service check pruning
  - Configurable retention policies and cleanup intervals
  - Cleanup statistics tracking in StorageActor
- **2025-01-16**: Phase 4.1 (API Server) complete
  - Full REST API with Axum framework
  - All endpoints implemented (health, stats, servers, services)
  - Bearer token authentication and CORS support
  - WebSocket streaming for real-time updates
- **2025-01-16**: Phase 4.2 (TUI Dashboard) complete
  - Three-tab interface (Servers, Services, Alerts)
  - Time-based charts with sliding window (HH:MM:SS labels)
  - Enhanced memory visualization with color-coded gauges
  - Historical data loading on startup
  - WebSocket integration with automatic reconnection
  - TOML configuration support with CLI overrides
  - All tests passing (84/84) âœ…

---

## Phase 5: Polish & Production Readiness ğŸš€ [ğŸ¯ IN PROGRESS]

**Goal:** Optimize, document, and prepare for production deployment

**Duration:** 1-2 weeks

**Status:** In progress - focus on v1.0.0 release âœ¨

### 5.1 Performance Optimization
- [ ] Profile CPU and memory usage under load
- [ ] Optimize database queries and indexes
- [ ] Implement connection pooling for SQLite
- [ ] Add caching layer for frequent queries (server list, service status)
- [ ] Benchmark metric throughput (target: 10k metrics/sec)
- [ ] Load testing with multiple agents and services

### 5.2 Observability
- [ ] Add structured logging with log levels (tracing/serde_json)
- [ ] Implement metrics about the monitoring system itself (meta-monitoring)
  - Actor health and message queue depths
  - Storage backend performance metrics
  - API request/response times
- [ ] Enhanced health check endpoints (storage, actors, connectivity)
- [ ] Add distributed tracing support (optional - opentelemetry)

### 5.3 Documentation
- [ ] Complete API documentation (OpenAPI/Swagger spec)
- [ ] Write deployment guides (systemd, Docker, Kubernetes)
- [ ] Create troubleshooting guide (common issues, debugging)
- [ ] Add example configurations (production, development, minimal)
- [ ] Record demo videos/screenshots for TUI dashboard
- [ ] Architecture diagrams (actor communication, data flow)

### 5.4 Distribution
- [ ] Create release binaries for major platforms (Linux, macOS, Windows)
- [ ] Docker images with multi-stage builds
  - Hub image
  - Agent image
  - All-in-one demo image
- [ ] Installation scripts (curl | bash installer)
- [ ] Homebrew formula (macOS)
- [ ] Package for apt/yum (Linux distributions)
- [ ] GitHub Actions for automated releases

### 5.5 Testing & Quality
- [x] Good unit test coverage (29 unit tests) âœ…
- [x] Integration tests for actor communication (43 tests) âœ…
- [x] Property-based tests (9 tests) âœ…
- [ ] Performance regression tests
- [ ] Chaos testing (network failures, high load, disk full)
- [ ] End-to-end tests (agent â†’ hub â†’ dashboard)
- [ ] Security audit (dependency scanning, SAST)

### 5.6 Configuration & UX
- [ ] Configuration validation with helpful error messages
- [ ] Migration tool for config format changes
- [ ] Environment variable support for sensitive values
- [ ] Wizard/interactive setup for first-time users
- [ ] Better CLI help and examples

**Dependencies:** Phase 1-4 âœ…
**Deliverables:** Production-ready v1.0.0 release with binaries and documentation
**Target:** Q1 2025

---

## Future Enhancements (v1.1.0+)

### Possible Features
- ğŸ“± Mobile app for monitoring on-the-go
- ğŸ”” Additional alert channels (Slack, PagerDuty, email)
- ğŸ“Š Web UI (alternative to TUI)
- ğŸ¤– Anomaly detection with ML
- ğŸ“ Custom metric plugins via WASM
- ğŸŒ Geo-distributed monitoring
- ğŸ“ˆ Custom dashboards and reports
- ğŸ” Multi-tenancy support
- ğŸ”„ Configuration management UI
- ğŸ¯ SLA tracking and reporting

---

## Timeline Summary

| Phase | Duration | Status | Completion Date | Notes |
|-------|----------|--------|-----------------|-------|
| Phase 1: Architecture | 1-2 weeks | âœ… COMPLETE | 2025-01-15 | Actor-based architecture with graceful shutdown |
| Phase 2: Persistence | 1-2 weeks | âœ… COMPLETE | 2025-01-15 | SQLite backend with batching and hybrid schema |
| Phase 3: Services | 1 week | âœ… COMPLETE | 2025-01-15 | HTTP/HTTPS monitoring with alerts and uptime |
| Phase 4.0: Retention | 2-3 days | âœ… COMPLETE | 2025-01-16 | Automatic cleanup with configurable policies |
| Phase 4.1: API Server | 1 week | âœ… COMPLETE | 2025-01-16 | REST API + WebSocket streaming |
| Phase 4.2: TUI Dashboard | 1 week | âœ… COMPLETE | 2025-01-16 | Ratatui dashboard with time-based charts |
| Phase 5: Polish | 1-2 weeks | ğŸ¯ IN PROGRESS | Target: Q1 2025 | Production readiness, optimization |
| Phase 3.5: Alert Refactoring | 3-5 days | ğŸ“‹ PLANNED | After v1.0.0 | Medium priority - split metric/service alerts |

**Progress (as of 2025-01-16):**
- âœ… Core features complete: All of Phases 1-4 (100%)
- âœ… Test coverage: 84 tests passing (29 unit + 43 integration + 9 property + 3 doc)
- ğŸ¯ Current: Phase 5 (Production hardening and optimization)
- ğŸ“‹ Target: v1.0.0 release in Q1 2025
- ğŸ“‹ Post-release: Phase 3.5 (Alert architecture refactoring)

---

## Success Metrics

**v1.0.0 Goals:**
- âœ… Zero-downtime metric collection (actor-based architecture implemented)
- âœ… Storage: Persistent SQLite backend with configurable retention
- âœ… Dashboard: Sub-second TUI responsiveness with real-time updates achieved
- âœ… API: WebSocket streaming with Bearer token authentication
- âœ… Services: HTTP/HTTPS health checks with uptime tracking
- âœ… Test Coverage: 84 tests (29 unit + 43 integration + 9 property + 3 doc)
- ğŸ¯ Reliability: 99.9% uptime for monitoring itself (needs production validation)
- ğŸ¯ Performance: 10k metrics/sec throughput (needs benchmarking in Phase 5)
- ğŸ¯ Documentation: Complete deployment guides and API docs (in progress)
- ğŸ¯ Distribution: Release binaries for Linux, macOS, Windows (planned)

---

## Risk Assessment

| Risk | Impact | Mitigation |
|------|--------|------------|
| Performance degradation with actor model | High | Thorough benchmarking in Phase 1 |
| Database scalability limits | Medium | Design for horizontal sharding from start |
| TUI complexity and bugs | Medium | Incremental UI development with testing |
| ICMP permission issues | Low | Clear documentation for capability setup |
| Backward compatibility breaks | Medium | Maintain config migration path |

---

## Contributing

This is a living document. As development progresses:
1. Check off completed items with âœ…
2. Add notes about implementation decisions
3. Update timelines based on actual progress
4. Document any deviations from the plan

For detailed technical specifications, see the linked documents in each phase.
